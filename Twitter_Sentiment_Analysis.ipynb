{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Twitter Sentiment Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Avisha-7/Twitter-Sentiment-Analysis/blob/main/Twitter_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIOIlGVUUm-n"
      },
      "source": [
        "# Loading the dataset from Google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZhLkmUkO59g"
      },
      "source": [
        "# Import PyDrive and associated libraries\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rxz1za_qBYRt"
      },
      "source": [
        "file_id = laggVyWshwcyP6kEI-y_W3P8D26sz\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "\n",
        "# Save file in Colab memory\n",
        "downloaded.GetContentFile('tweet_data.csv')  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8IZvvYyA9sr"
      },
      "source": [
        "# Preprocessing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-K4rZaHvkJhd"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMVYdAbhkSEv"
      },
      "source": [
        "df = pd.read_csv(\"tweet_data.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_ePfKnQkiHh"
      },
      "source": [
        "# checking data entry\n",
        "#df.sample(10) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-d8KD2RCzBMv"
      },
      "source": [
        "# Verifying the total number of tweets\n",
        "# print(\"Number of tweets: {}\".format(len(df)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2QDfi2dWWFK"
      },
      "source": [
        "# Dataset Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fb79SIovB06O"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ya7k2TcylAbp"
      },
      "source": [
        "sentiment_count = df[\"sentiment\"].value_counts()\n",
        "plt.pie(sentiment_count, labels=sentiment_count.index,\n",
        "        autopct='%1.1f%%', shadow=True, startangle=140)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlwWqCohC87z"
      },
      "source": [
        "print(\"Number of + tweets: {}\".format(df[df[\"sentiment\"]==\"positive\"].count()[0]))\n",
        "print(\"Number of - tweets: {}\".format(df[df[\"sentiment\"]==\"negative\"].count()[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiME9k4uBxix"
      },
      "source": [
        "from wordcloud import WordCloud"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fW3YNWHwwYdP"
      },
      "source": [
        "# to visualise the most recurrent words in the text corpus with positive sentiment\n",
        "\n",
        "pos_tweets = df[df[\"sentiment\"]==\"positive\"]\n",
        "txt = \" \".join(tweet.lower() for tweet in pos_tweets[\"tweet_text\"])\n",
        "wordcloud = WordCloud().generate(txt)\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbHSPX8jE2Jk"
      },
      "source": [
        "# to visualise the most recurrent words in the text corpus with negative sentiment\n",
        "\n",
        "neg_tweets = df[df[\"sentiment\"]==\"negative\"]\n",
        "txt = \" \".join(tweet.lower() for tweet in neg_tweets[\"tweet_text\"])\n",
        "wordcloud = WordCloud().generate(txt)\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIFAbVfhMvKS"
      },
      "source": [
        "# Text Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qefx0Ad1RFHy"
      },
      "source": [
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMe1sJQZWBon"
      },
      "source": [
        "# handle RT tag by replacing occurences of RT with a default value, i.e., \" \"\n",
        "def replace_retweet(tweet, default_replace=\"\"):\n",
        "  tweet = re.sub('RT\\s+', default_replace, tweet)\n",
        "  return tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIFwuvPQI5L4"
      },
      "source": [
        "print(\"Processed tweet: {}\".format(replace_retweet(tweet)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODnCCX1ueIwO"
      },
      "source": [
        "#handle user tag by replacing @ with default value\n",
        "def replace_user(tweet, default_replace=\"twitteruser\"):\n",
        "  tweet = re.sub('\\B@\\w+', default_replace, tweet)\n",
        "  return tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgSircywI8fG"
      },
      "source": [
        "print(\"Processed tweet: {}\".format(replace_user(tweet)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDtbHjDiiaw7"
      },
      "source": [
        "pip install emoji --upgrade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8Ctx6FUiVyc"
      },
      "source": [
        "import emoji"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqHXnwRZn2Ey"
      },
      "source": [
        "# handle emojis by replacing them by meaningful text\n",
        "def demojize(tweet):\n",
        "  tweet = emoji.demojize(tweet)\n",
        "  return tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeuCWELwx2bb"
      },
      "source": [
        "print(\"Processed tweet: {}\".format(demojize(tweet)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Vq3d5VcfdCc"
      },
      "source": [
        "# handle url by replacing http:// or https:// with a default value\n",
        "def replace_url(tweet, default_replace=\"\"):\n",
        "  tweet = re.sub('(http|https):\\/\\/\\S+', default_replace, tweet)\n",
        "  return tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YN2c4pJIy8pX"
      },
      "source": [
        "print(\"Processed tweet: {}\".format(replace_url(tweet)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yI76Ve3lg3WZ"
      },
      "source": [
        "# handle hashtag by replacing occurrences of #value\n",
        "def replace_hashtag(tweet, default_replace=\"\"):\n",
        "  tweet = re.sub('#+', default_replace, tweet)\n",
        "  return tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xlMKyD4y-Xg"
      },
      "source": [
        "print(\"Processed tweet: {}\".format(replace_hashtag(tweet)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xw18TAaFhOMH"
      },
      "source": [
        "tweet = \"LOOOOOOOOK at this ... I'd like it so much!\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXCITDNeQtnE"
      },
      "source": [
        "# converting tweets to lowercase\n",
        "def to_lowercase(tweet):\n",
        "  tweet = tweet.lower()\n",
        "  return tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Roo8SHo23vw"
      },
      "source": [
        "print(\"Processed tweet: {}\".format(to_lowercase(tweet)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USLuHaaiS9xm"
      },
      "source": [
        "#handling repitition of characters \n",
        "def word_repetition(tweet):\n",
        "  tweet = re.sub(r'(.)\\1+', r'\\1\\1', tweet)\n",
        "  return tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkWh38hH5qyY"
      },
      "source": [
        "print(\"Processed tweet: {}\".format(word_repetition(tweet)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_awIiJ_ngo7"
      },
      "source": [
        "#handling repitition of punctuations\n",
        "def punct_repetition(tweet, default_replace=\"\"):\n",
        "  tweet = re.sub(r'[\\?\\.\\!]+(?=[\\?\\.\\!])', default_replace, tweet)\n",
        "  return tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfRfogEe5str"
      },
      "source": [
        "print(\"Processed tweet: {}\".format(punct_repetition(tweet)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wO2Gle7Ey360"
      },
      "source": [
        "pip install contractions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdkPTEZryzJC"
      },
      "source": [
        "import contractions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DwhVxKs1QL_"
      },
      "source": [
        "print(contractions.contractions_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1xj5UXbVspq"
      },
      "source": [
        "# function to replace contractions with their extended forms \n",
        "def _fix_contractions(tweet):\n",
        "  for k, v in contractions.contractions_dict.items():\n",
        "    tweet = tweet.replace(k, v)\n",
        "  return tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MDFOXKTccRV"
      },
      "source": [
        "print(\"Processed tweet: {}\".format(_fix_contractions(tweet)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQijW6z65IzJ"
      },
      "source": [
        "* Create a `_fix_contractions` function used to replace contractions with their extended forms by using the contractions package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MP6atIK-VsNw"
      },
      "source": [
        "# function to replace contractions with their extended forms \n",
        "def fix_contractions(tweet):\n",
        "  tweet = contractions.fix(tweet)\n",
        "  return tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqQ60KaMc0CS"
      },
      "source": [
        "print(\"Processed tweet: {}\".format(fix_contractions(tweet)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7Qnxa-_2q6x"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnKD-MkZ2kNd"
      },
      "source": [
        "pip install nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6SVKvVP4NDL"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MthNx7ou4OnW"
      },
      "source": [
        "def tokenize(tweet):\n",
        "  tokens = word_tokenize(tweet)\n",
        "  return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPdJeqcRCYT2"
      },
      "source": [
        "print(type(tokenize(tweet)))\n",
        "print(\"Tweet tokens: {}\".format(tokenize(tweet)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baAGSohY5PcV"
      },
      "source": [
        "#custom tokenization\n",
        "import string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yc6naeSUkNs8"
      },
      "source": [
        "print(string.punctuation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MBBQJDeQSi4"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZXqXrG42qMk"
      },
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "print(stop_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kcgpw29wmec4"
      },
      "source": [
        "stop_words.discard('not')\n",
        "print(stop_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6cZbaTB5Yr7"
      },
      "source": [
        "def custom_tokenize(tweet,\n",
        "                    keep_punct = False,\n",
        "                    keep_alnum = False,\n",
        "                    keep_stop = False):\n",
        "  \n",
        "  token_list = word_tokenize(tweet)\n",
        "\n",
        "  if not keep_punct:\n",
        "    token_list = [token for token in token_list\n",
        "                  if token not in string.punctuation]\n",
        "\n",
        "  if not keep_alnum:\n",
        "    token_list = [token for token in token_list if token.isalpha()]\n",
        "  \n",
        "  if not keep_stop:\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    stop_words.discard('not')\n",
        "    token_list = [token for token in token_list if not token in stop_words]\n",
        "\n",
        "  return token_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXyaftGrEvkC"
      },
      "source": [
        "print(\"Tweet tokens: {}\".format(custom_tokenize(tweet, \n",
        "                                                keep_punct=True, \n",
        "                                                keep_alnum=True, \n",
        "                                                keep_stop=True)))\n",
        "print(\"Tweet tokens: {}\".format(custom_tokenize(tweet, keep_stop=True)))\n",
        "print(\"Tweet tokens: {}\".format(custom_tokenize(tweet, keep_alnum=True)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIY4CKCcTyEo"
      },
      "source": [
        "# Stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oaU_cd_Wz0u"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "from nltk.stem.snowball import SnowballStemmer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8QDL-05tMj5"
      },
      "source": [
        "porter_stemmer = PorterStemmer()\n",
        "lancaster_stemmer = LancasterStemmer()\n",
        "snoball_stemmer = SnowballStemmer('english')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CetNdOeRTyjj"
      },
      "source": [
        "# function to take the list of tokens as input and returns a list of stemmed tokens\n",
        "def stem_tokens(tokens, stemmer):\n",
        "  token_list = []\n",
        "  for token in tokens:\n",
        "    token_list.append(stemmer.stem(token))\n",
        "  return token_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDZqSc43tKjI"
      },
      "source": [
        "print(\"Porter stems: {}\".format(stem_tokens(tokens, porter_stemmer)))\n",
        "print(\"Lancaster stems: {}\".format(stem_tokens(tokens, lancaster_stemmer)))\n",
        "print(\"Snowball stems: {}\".format(stem_tokens(tokens, snoball_stemmer)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGGhCVph1VDp"
      },
      "source": [
        "# trying different functions\n",
        "print(\"Porter stems: {}\".format(stem_tokens(tokens, porter_stemmer)))\n",
        "print(\"Lancaster stems: {}\".format(stem_tokens(tokens, lancaster_stemmer)))\n",
        "print(\"Snowball stems: {}\".format(stem_tokens(tokens, snoball_stemmer)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62AsuQu6UIEn"
      },
      "source": [
        "# Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUdQlhSQUL7T"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-AAGLN4UWTO"
      },
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6Kiw9xOZE-F"
      },
      "source": [
        "def lemmatize_tokens(tokens, word_type, lemmatizer):\n",
        "  token_list = []\n",
        "  for token in tokens:\n",
        "    token_list.append(lemmatizer.lemmatize(token, word_type[token]))\n",
        "  return token_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nnvMrk_UYZ1"
      },
      "source": [
        "print(\"Tweet lemma: {}\".format(\n",
        "    lemmatize_tokens(tokens, word_type, lemmatizer)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiQofxMv37Ik"
      },
      "source": [
        "# function to process tweets end-to-end (compiling together)\n",
        "def process_tweet(tweet, verbose=False):\n",
        "  if verbose: print(\"Initial tweet: {}\".format(tweet))\n",
        "\n",
        "  ## Twitter Features\n",
        "  tweet = replace_retweet(tweet) # replace retweet\n",
        "  tweet = replace_user(tweet, \"\") # replace user tag\n",
        "  tweet = replace_url(tweet) # replace url\n",
        "  tweet = replace_hashtag(tweet) # replace hashtag\n",
        "  if verbose: print(\"Post Twitter processing tweet: {}\".format(tweet))\n",
        "\n",
        "  ## Word Features\n",
        "  tweet = to_lowercase(tweet) # lower case\n",
        "  tweet = fix_contractions(tweet) # replace contractions\n",
        "  tweet = punct_repetition(tweet) # replace punctuation repetition\n",
        "  tweet = word_repetition(tweet) # replace word repetition\n",
        "  tweet = demojize(tweet) # replace emojis\n",
        "  if verbose: print(\"Post Word processing tweet: {}\".format(tweet))\n",
        "\n",
        "  ## Tokenization & Stemming\n",
        "  tokens = custom_tokenize(tweet, keep_alnum=False, keep_stop=False) # tokenize\n",
        "  stemmer = SnowballStemmer(\"english\") # define stemmer\n",
        "  stem = stem_tokens(tokens, stemmer) # stem tokens\n",
        "\n",
        "  return stem"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Xb146QF4uhj"
      },
      "source": [
        "print(process_tweet(complex_tweet, verbose=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26lxJnf2K7zd"
      },
      "source": [
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBa0HUtM4x0n"
      },
      "source": [
        "for i in range(5):\n",
        "  tweet_id = random.randint(0,len(df))\n",
        "  tweet = df.iloc[tweet_id][\"tweet_text\"]\n",
        "  print(process_tweet(tweet, verbose=True))\n",
        "  print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1ScK4-f_cJB"
      },
      "source": [
        "# Text Representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgEa9cqUzoEK"
      },
      "source": [
        "Processing Tweets\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1oZA6AjqC3X"
      },
      "source": [
        "pip install -U scikit-learn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0_4R6tNqcAg"
      },
      "source": [
        "df[\"tokens\"] = df[\"tweet_text\"].apply(process_tweet)\n",
        "df[\"tweet_sentiment\"] = df[\"sentiment\"].apply(lambda i: 1\n",
        "                                              if i == \"positive\" else 0)\n",
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ru_1XwdIqgXH"
      },
      "source": [
        "# converting DataFrame to two lists: one for the tweet tokens (X) and one for the tweet sentiment (y)\n",
        "X = df[\"tokens\"].tolist()\n",
        "y = df[\"tweet_sentiment\"].tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dm6k_K81i1cG"
      },
      "source": [
        "print(X)\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSSP3EFyp9Nr"
      },
      "source": [
        "# function used to build a dictionary with the word and sentiment as index and the count of occurence as value\n",
        "def build_freqs(tweet_list, sentiment_list):\n",
        "  freqs = {}\n",
        "  for tweet, sentiment in zip(tweet_list, sentiment_list):\n",
        "    for word in tweet:\n",
        "      pair = (word, sentiment)\n",
        "      if pair in freqs:\n",
        "        freqs[pair] += 1\n",
        "      else:\n",
        "        freqs[pair] = 1\n",
        "  return freqs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S58DffMwi5vN"
      },
      "source": [
        "freqs = build_freqs(corpus, sentiment)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pb0BvLORtJql"
      },
      "source": [
        "print(freqs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vnVv4TOi63Y"
      },
      "source": [
        "freqs_all = build_freqs(X, y) #entire dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvafc_1fsktr"
      },
      "source": [
        "print(\"Frequency of word 'love' in + tweets: {}\".format(freqs_all[(\"love\", 1)]))\n",
        "print(\"Frequency of word 'love' in - tweets: {}\".format(freqs_all[(\"love\", 0)]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QoyYL6wtTBP"
      },
      "source": [
        "def tweet_to_freq(tweet, freqs):\n",
        "  x = np.zeros((2,))\n",
        "  for word in tweet:\n",
        "    if (word, 1) in freqs:\n",
        "      x[0] += freqs[(word, 1)]\n",
        "    if (word, 0) in freqs:\n",
        "      x[1] += freqs[(word, 0)]\n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JL3E5vRrxJrZ"
      },
      "source": [
        "fig, ax = plt.subplots(figsize = (8, 8))\n",
        "\n",
        "word1 = \"happi\"\n",
        "word2 = \"sad\"\n",
        "\n",
        "def word_features(word, freqs):\n",
        "  x = np.zeros((2,))\n",
        "  if (word, 1) in freqs:\n",
        "    x[0] = np.log(freqs[(word, 1)] + 1)\n",
        "  if (word, 0) in freqs:\n",
        "    x[1] = np.log(freqs[(word, 0)] + 1)\n",
        "  return x\n",
        "\n",
        "x_axis = [word_features(word, freqs_all)[0] for word in [word1, word2]]\n",
        "y_axis = [word_features(word, freqs_all)[1] for word in [word1, word2]]\n",
        "\n",
        "ax.scatter(x_axis, y_axis)  \n",
        "\n",
        "plt.xlabel(\"Log Positive count\")\n",
        "plt.ylabel(\"Log Negative count\")\n",
        "\n",
        "ax.plot([0, 9], [0, 9], color = 'red')\n",
        "plt.text(x_axis[0], y_axis[0], word1)\n",
        "plt.text(x_axis[1], y_axis[1], word2)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_a_uGCpQ_nu0"
      },
      "source": [
        "Bag of Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjjZkgHCpw_m"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEzNadu6pwYh"
      },
      "source": [
        "# function used to build the Bag-of-Words vectorizer with the corpus\n",
        "def fit_cv(tweet_corpus):\n",
        "  cv_vect = CountVectorizer(tokenizer=lambda x: x,\n",
        "                            preprocessor=lambda x: x)\n",
        "  cv_vect.fit(tweet_corpus)\n",
        "  return cv_vect"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xed7_qlKwoX9"
      },
      "source": [
        "cv_vect = fit_cv(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJcalFLxjIEx"
      },
      "source": [
        "ft = cv_vect.get_feature_names()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-nxkR7VfWii"
      },
      "source": [
        "print(\"There are {} features in this corpus\".format(len(ft)))\n",
        "print(ft)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZ-mcZScBdtb"
      },
      "source": [
        "cv_mtx = cv_vect.transform(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXm86BaJp0yB"
      },
      "source": [
        "print(\"Matrix shape is: {}\".format(cv_mtx.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBH3Hy60gQB4"
      },
      "source": [
        "cv_mtx.toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sa_FL6vn79E"
      },
      "source": [
        "Term Frequency – Inverse Document Frequency (TF-IDF)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlBvj6pARLH1"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NUd39ZsRQPI"
      },
      "source": [
        "# function used to build the TF-IDF vectorizer with the corpus\n",
        "def fit_tfidf(tweet_corpus):\n",
        "  tf_vect = TfidfVectorizer(preprocessor=lambda x: x,\n",
        "                            tokenizer=lambda x: x)\n",
        "  tf_vect.fit(tweet_corpus)\n",
        "  return tf_vect"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjmZhliftA_Q"
      },
      "source": [
        "tf_vect = fit_tfidf(corpus)\n",
        "tf_mtx = tf_vect.transform(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRqHhf3njSEe"
      },
      "source": [
        "ft = tf_vect.get_feature_names()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tywqkwWPlYpc"
      },
      "source": [
        "print(\"There are {} features in this corpus\".format(len(ft)))\n",
        "print(ft)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtamuqDjlxmE"
      },
      "source": [
        "print(tf_mtx.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVVbtSxqlxVi"
      },
      "source": [
        "tf_mtx.toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7H4tg1lhoBxR"
      },
      "source": [
        "\n",
        "# Sentiment Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBiVy9LSnlVN"
      },
      "source": [
        "import seaborn as sn\n",
        "\n",
        "def plot_confusion(cm):\n",
        "  plt.figure(figsize = (5,5))\n",
        "  sn.heatmap(cm, annot=True, cmap=\"Blues\", fmt='.0f')\n",
        "  plt.xlabel(\"Prediction\")\n",
        "  plt.ylabel(\"True value\")\n",
        "  plt.title(\"Confusion Matrix\")\n",
        "  return sn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePstP2McY2Ql"
      },
      "source": [
        "Train/ Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMqL7yh1Dt8t"
      },
      "source": [
        "print(X)\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkeEyh9TeTT3"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdKNxWTXDvLM"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    random_state=0,\n",
        "                                                    train_size=0.80)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fUaBFavoBTG"
      },
      "source": [
        "print(\"Size of X_train: {}\".format(len(X_train)))\n",
        "print(\"Size of y_train: {}\".format(len(y_train)))\n",
        "print(\"\\n\")\n",
        "print(\"Size of X_test: {}\".format(len(X_test)))\n",
        "print(\"Size of y_test: {}\".format(len(y_test)))\n",
        "print(\"\\n\")\n",
        "print(\"Train proportion: {:.0%}\".format(len(X_train)/\n",
        "                                        (len(X_train)+len(X_test))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5nWVNZzobfd"
      },
      "source": [
        "id = random.randint(0,len(X_train))\n",
        "print(\"Train tweet: {}\".format(X_train[id]))\n",
        "print(\"Sentiment: {}\".format(y_train[id]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pV5EWbQpIOp"
      },
      "source": [
        "# Logistic Regression\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vvELqpBA62u"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVCZ2jEcb_Kx"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IN8x-HdwcC1K"
      },
      "source": [
        "def fit_lr(X_train, y_train):\n",
        "  model = LogisticRegression()\n",
        "  model.fit(X_train, y_train)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWD8LNbJDHG9"
      },
      "source": [
        "Frequency\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKDhO2RKBafD"
      },
      "source": [
        "freqs = build_freqs(X_train, y_train)\n",
        "X_train_pn = [tweet_to_freq(tweet, freqs) for tweet in X_train]\n",
        "X_test_pn = [tweet_to_freq(tweet, freqs) for tweet in X_test]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgXgI2QcjYyv"
      },
      "source": [
        "model_lr_pn = fit_lr(X_train_pn, y_train)\n",
        "print(model_lr_pn.coef_, model_lr_pn.intercept_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZtyVpIDZBRr"
      },
      "source": [
        "Count Vector\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXba4ZPaYsb5"
      },
      "source": [
        "cv = fit_cv(X_train)\n",
        "X_train_cv = cv.transform(X_train)\n",
        "X_test_cv = cv.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xarmCHAIZMty"
      },
      "source": [
        "model_lr_cv = fit_lr(X_train_cv, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vo7WerftZV4t"
      },
      "source": [
        "TF-IDF\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2Ex6PkLZXwC"
      },
      "source": [
        "tf = fit_tfidf(X_train)\n",
        "X_train_tf = tf.transform(X_train)\n",
        "X_test_tf = tf.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfXA_WE7ZgCO"
      },
      "source": [
        "model_lr_tf = fit_lr(X_train_tf, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTZWCw7CuAqy"
      },
      "source": [
        "# Performance Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pv_HXA9ttu4I"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-OBeBb4vf-Y"
      },
      "source": [
        "Positive/Negative frequencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGSRknqlj6fu"
      },
      "source": [
        "y_pred_lr_pn = model_lr_pn.predict(X_test_pn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4Mw_VHwj6Qp"
      },
      "source": [
        "print(\"LR Model Accuracy: {:.2%}\".format(accuracy_score(y_test, y_pred_lr_pn)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMFVLs_vAO03"
      },
      "source": [
        "plot_confusion(confusion_matrix(y_test, y_pred_lr_pn))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suYa_OYgZPq1"
      },
      "source": [
        "y_pred_lr_cv = model_lr_cv.predict(X_test_cv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAOncwPekK1_"
      },
      "source": [
        "print(\"LR Model Accuracy: {:.2%}\".format(accuracy_score(y_test, y_pred_lr_cv)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlaWIWnNkL1O"
      },
      "source": [
        "plot_confusion(confusion_matrix(y_test, y_pred_lr_cv))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4MmLzcCZvto"
      },
      "source": [
        "y_pred_lr_tf = model_lr_tf.predict(X_test_tf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GD6mO1eJkV3m"
      },
      "source": [
        "print(\"LR Model Accuracy: {:.2%}\".format(accuracy_score(y_test, y_pred_lr_tf)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kt_FBSmbkW0b"
      },
      "source": [
        "plot_confusion(confusion_matrix(y_test, y_pred_lr_tf))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}